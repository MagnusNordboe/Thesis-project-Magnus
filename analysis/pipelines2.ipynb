{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sktime\n",
    "import seaborn as snsc\n",
    "import matplotlib.pyplot as plt\n",
    "from convertcsv.import_preprocess_v4 import readcsvs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from sktime.transformations.panel.rocket import MiniRocketMultivariate\n",
    "from sktime.classification.kernel_based import RocketClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.impute import KNNImputer\n",
    "from convertcsv.get_all_metrics_with_tags import get_all_metrics_with_tags\n",
    "from visualization import graphs\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sktime.datatypes import convert_to\n",
    "from sktime.datatypes import MTYPE_REGISTER\n",
    "from collections import Counter\n",
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sktime.classification.compose import ClassifierPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What should we do here?\n",
    "\n",
    "#Set up many different pipelines to compare.\n",
    "\n",
    "#The things to vary, in order of significance/importance\n",
    "\"\"\"\n",
    "NaN treshold\n",
    "\"\"\"\n",
    "file_list, y = get_all_metrics_with_tags(r\"F:\\Master\\Kubernetes\\sockshop\\microservices-demo\\query\\automated\\generated_csvs_4\")\n",
    "initial_readings = readcsvs(file_list, reduce_NaNs_treshold=False, remove_unique_cols=True, remove_monotonic_increasing=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prior testing revealed optimal KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=1)\n",
    "complete_value_set = imputer.fit_transform(initial_readings)\n",
    "imputed_df = pd.DataFrame(complete_value_set, index=initial_readings.index, columns=initial_readings.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into training x and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(convert_to(imputed_df,to_type=\"df-list\"), y)\n",
    "#X_train = convert_to(X_train, to_type=\"pd-multiindex\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding/reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the length of each individual time series\n",
    "def trimming(df:pd.DataFrame, y, min_percent=90):\n",
    "    instances = df.index.get_level_values(0).unique()\n",
    "    timeSeriesLengths = []\n",
    "    for instance in instances:\n",
    "        #This line of code gets every second level index value that corresponds to the current first level index value. \n",
    "        second_level_values = df.index.get_level_values(1)[df.index.get_level_values(0)==instance]\n",
    "        length = len(second_level_values)\n",
    "        timeSeriesLengths.append(length)\n",
    "    #Find the most common value.\n",
    "    counts = Counter(timeSeriesLengths)\n",
    "    most_common_value, most_common_value_count = counts.most_common(1)[0]\n",
    "    percentage = (most_common_value_count / len(timeSeriesLengths)) *100\n",
    "\n",
    "    if percentage >= min_percent:\n",
    "        indicies_to_drop = []\n",
    "        for indice in range(len(timeSeriesLengths)):\n",
    "            if timeSeriesLengths[indice] != most_common_value:\n",
    "                indicies_to_drop.append(indice)\n",
    "        filtered_df:pd.DataFrame = df[~df.index.get_level_values(0).isin(indicies_to_drop)]\n",
    "        #Reset the index\n",
    "        current_indice = 0\n",
    "        desired_indice = 0\n",
    "        newindex_tuples = []\n",
    "        for index_tuple in filtered_df.index:\n",
    "            #The thing to watch out for is the fact that we have to keep track of both the \n",
    "            #desired index and the index being displayed in the tuple. \n",
    "            #When the tuple being looked at changes, the index increases.\n",
    "            #If two indexes have been removed, the issue becomes keeping track of that. \n",
    "            #do a while loop to update it immediately. \n",
    "            #nevermind just set it equals, we only care about change anyway\n",
    "            if(index_tuple[0] != current_indice):\n",
    "                #this way, when the index being looked at changes, we know.\n",
    "                current_indice = index_tuple[0]\n",
    "                #we logged that there is a change so we know its time for next indice\n",
    "                desired_indice += 1\n",
    " \n",
    "            newindex_tuples.append((desired_indice, index_tuple[1]))\n",
    "        \n",
    "        newindex = pd.MultiIndex.from_tuples(newindex_tuples, names=filtered_df.index.names)\n",
    "        newdf = pd.DataFrame(filtered_df.values, columns=filtered_df.columns, index=newindex)\n",
    "            \n",
    "        y = np.array(y)\n",
    "\n",
    "        #Now rebuild the multiindex to be omontonically increasing.\n",
    "        \n",
    "        \n",
    "        #This still keeps the fucky index in the thing\n",
    "\n",
    "        return newdf, np.delete(y, indicies_to_drop)\n",
    "\n",
    "    return ValueError(percentage)\n",
    "    #To properly preprocess, one should check if the most common value equals the max value. If it does, great.\n",
    "    #Imagine we find a perfect thing for it. Now we have a couple of datasets consisting of multivariate data.\n",
    "    #It's important to preserve the general shape of the dataset. So just adding mean to the front and back probably isn't that good. \n",
    "    #Can decide to simply purge the ones that aren't of the correct length. To do this, you would have to decide that the most common value is the highest one, and that there are very few time series that differ. Incurs data loss but saves a lot of time.\n",
    "    #Instead of getting bogged down in a custom optimal solution, get the alright one. For now, get a determiner that purging the ones that are too small is ok. \n",
    "    #If ok, proceed. Can then later compare with other algorithms to see if the purging was helpful.\n",
    "    #Write in the thesis that shit happens and time constraints wcyd\n",
    "\n",
    "    #For now, determine if purging is ait\n",
    "    #The most common value should be at least 90% to only tolerate 10% data loss perhaps.\n",
    "\n",
    "trimmed_df , trimmed_y = trimming(imputed_df, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column in dataframe:\n",
    "#if column.chaos < treshold:\n",
    "#   remove column\n",
    "# \n",
    "#Get the average value (after normalization) for each column\n",
    "#If they are very similar, remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "converted = convert_to(trimmed_df,to_type=\"df-list\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(convert_to(trimmed_df,to_type=\"df-list\"),trimmed_y)\n",
    "X_train_mi = convert_to(X_train, to_type=\"pd-multiindex\")\n",
    "X_test_mi = convert_to(X_test, to_type='pd-multiindex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sktime.registry import all_tags\n",
    "# from sktime.registry import all_estimators\n",
    "# all_estimators(\"classifier\", filter_tags={\"capability:multivariate\":True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testscale = StandardScaler()\n",
    "scaled = testscale.fit_transform(trimmed_df)\n",
    "scaled = pd.DataFrame(data=scaled, index = X_train_mi.index, columns=X_train_mi.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Try_Classifiers:\n",
    "    #Goal: run the various classifiers. Do Normalization/standarization outside of this function.\n",
    "    #Then in here, only classifiers should be needed. All classifiers implement fit and fit_transform as well as fit_predict.\n",
    "    #If there are multiple classes that passed in, make the pipeline fit_transform -> fit_predict. Otherwise just fit_predict.\n",
    "    \n",
    "    def __init__(self, X_train:pd.DataFrame, y_train=None, X_test=None, y_test=None):\n",
    "        self.formats = [self.format_dflist, self.format_multiindex, self.format_multiindex_manual, self.format_multiindex_dflist]\n",
    "        self.X_train_cols = X_train.columns\n",
    "        self.X_train_ind = X_train.index\n",
    "        self.X_train = X_train\n",
    "        if y_train is not None:\n",
    "            self.y_train = y_train\n",
    "        if X_test is not None:\n",
    "            self.X_test = X_test\n",
    "        if y_test is not None:\n",
    "            self.y_test = y_test\n",
    "\n",
    "\n",
    "    def _try_function_with_formats(self, class_function, data):\n",
    "        for fmt in self.formats:\n",
    "            try:\n",
    "                formatted_data = fmt(data)\n",
    "                result = class_function(formatted_data)\n",
    "                return result, True\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        return None, False\n",
    "    \n",
    "    def format_dflist(self, input):\n",
    "        return convert_to(input, to_type=\"df-list\")\n",
    "\n",
    "    def format_multiindex(self, input):\n",
    "        return convert_to(input, to_type=\"pd-multiindex\")\n",
    "\n",
    "    def format_multiindex_manual(self, input):\n",
    "        return pd.DataFrame(input, columns=self.X_train_cols, index=self.X_train_ind)\n",
    "    \n",
    "    def format_multiindex_dlflist(self, input):\n",
    "        return convert_to(pd.DataFrame(input, columns=self.X_train_cols, index=self.X_train_ind), to_type=\"df-list\")\n",
    "\n",
    "    def run_fit_predict_single(self, class_to_use):\n",
    "        \n",
    "        result, completed = self._try_function_with_formats(class_to_use.fit(self.X_train, self.y_train))\n",
    "        prediction = self._try_function_with_formats(class_to_use.predict(self.X_test))\n",
    "        \n",
    "        if completed:\n",
    "            return result\n",
    "        return Exception(\"Something went wrong\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
