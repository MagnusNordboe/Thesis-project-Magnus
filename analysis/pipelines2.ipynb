{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sktime\n",
    "import seaborn as snsc\n",
    "import matplotlib.pyplot as plt\n",
    "from convertcsv.import_preprocess_v4 import readcsvs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from sktime.transformations.panel.rocket import MiniRocketMultivariate\n",
    "from sktime.classification.kernel_based import RocketClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.impute import KNNImputer\n",
    "from convertcsv.get_all_metrics_with_tags import get_all_metrics_with_tags\n",
    "from visualization import graphs\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sktime.datatypes import convert_to\n",
    "from sktime.datatypes import MTYPE_REGISTER\n",
    "from collections import Counter\n",
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sktime.classification.compose import ClassifierPipeline\n",
    "from storage.retrieve_csv import retrieve_csv\n",
    "from storage.store_csv import store_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What should we do here?\n",
    "\n",
    "#Set up many different pipelines to compare.\n",
    "\n",
    "#The things to vary, in order of significance/importance\n",
    "\"\"\"\n",
    "NaN treshold\n",
    "\"\"\"\n",
    "file_list, y = get_all_metrics_with_tags(r\"F:\\Master\\Kubernetes\\sockshop\\microservices-demo\\query\\automated\\generated_csvs_4\")\n",
    "initial_readings = readcsvs(file_list, reduce_NaNs_treshold=False, remove_unique_cols=True, remove_monotonic_increasing=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prior testing revealed optimal KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=1)\n",
    "complete_value_set = imputer.fit_transform(initial_readings)\n",
    "imputed_df = pd.DataFrame(complete_value_set, index=initial_readings.index, columns=initial_readings.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into training x and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(convert_to(imputed_df,to_type=\"df-list\"), y)\n",
    "#X_train = convert_to(X_train, to_type=\"pd-multiindex\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding/reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function removes features that have nearly static values\n",
    "def trimming(df:pd.DataFrame, y, min_percent=90):\n",
    "    instances = df.index.get_level_values(0).unique()\n",
    "    timeSeriesLengths = []\n",
    "    for instance in instances:\n",
    "        #This line of code gets every second level index value that corresponds to the current first level index value. \n",
    "        second_level_values = df.index.get_level_values(1)[df.index.get_level_values(0)==instance]\n",
    "        length = len(second_level_values)\n",
    "        timeSeriesLengths.append(length)\n",
    "    #Find the most common value.\n",
    "    counts = Counter(timeSeriesLengths)\n",
    "    most_common_value, most_common_value_count = counts.most_common(1)[0]\n",
    "    percentage = (most_common_value_count / len(timeSeriesLengths)) *100\n",
    "\n",
    "    if percentage >= min_percent:\n",
    "        indicies_to_drop = []\n",
    "        for indice in range(len(timeSeriesLengths)):\n",
    "            if timeSeriesLengths[indice] != most_common_value:\n",
    "                indicies_to_drop.append(indice)\n",
    "        filtered_df:pd.DataFrame = df[~df.index.get_level_values(0).isin(indicies_to_drop)]\n",
    "        #Reset the index\n",
    "        current_indice = 0\n",
    "        desired_indice = 0\n",
    "        newindex_tuples = []\n",
    "        for index_tuple in filtered_df.index:\n",
    "            #The thing to watch out for is the fact that we have to keep track of both the \n",
    "            #desired index and the index being displayed in the tuple. \n",
    "            #When the tuple being looked at changes, the index increases.\n",
    "            #If two indexes have been removed, the issue becomes keeping track of that. \n",
    "            #do a while loop to update it immediately. \n",
    "            #nevermind just set it equals, we only care about change anyway\n",
    "            if(index_tuple[0] != current_indice):\n",
    "                #this way, when the index being looked at changes, we know.\n",
    "                current_indice = index_tuple[0]\n",
    "                #we logged that there is a change so we know its time for next indice\n",
    "                desired_indice += 1\n",
    " \n",
    "            newindex_tuples.append((desired_indice, index_tuple[1]))\n",
    "        \n",
    "        newindex = pd.MultiIndex.from_tuples(newindex_tuples, names=filtered_df.index.names)\n",
    "        newdf = pd.DataFrame(filtered_df.values, columns=filtered_df.columns, index=newindex)\n",
    "            \n",
    "        y = np.array(y)\n",
    "\n",
    "        #Now rebuild the multiindex to be ontologically increasing.\n",
    "        \n",
    "        \n",
    "        #This still keeps the fucky index in the thing\n",
    "\n",
    "        return newdf, np.delete(y, indicies_to_drop)\n",
    "\n",
    "    return ValueError(percentage)\n",
    "    #To properly preprocess, one should check if the most common value equals the max value. If it does, great.\n",
    "    #Imagine we find a perfect thing for it. Now we have a couple of datasets consisting of multivariate data.\n",
    "    #It's important to preserve the general shape of the dataset. So just adding mean to the front and back probably isn't that good. \n",
    "    #Can decide to simply purge the ones that aren't of the correct length. To do this, you would have to decide that the most common value is the highest one, and that there are very few time series that differ. Incurs data loss but saves a lot of time.\n",
    "    #Instead of getting bogged down in a custom optimal solution, get the alright one. For now, get a determiner that purging the ones that are too small is ok. \n",
    "    #If ok, proceed. Can then later compare with other algorithms to see if the purging was helpful.\n",
    "    #Write in the thesis that shit happens and time constraints wcyd\n",
    "\n",
    "    #For now, determine if purging is ait\n",
    "    #The most common value should be at least 90% to only tolerate 10% data loss perhaps.\n",
    "\n",
    "trimmed_df , trimmed_y = trimming(imputed_df, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column in dataframe:\n",
    "#if column.chaos < treshold:\n",
    "#   remove column\n",
    "# \n",
    "#Get the average value (after normalization) for each column\n",
    "#If they are very similar, remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "converted = convert_to(trimmed_df,to_type=\"df-list\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(convert_to(trimmed_df,to_type=\"df-list\"),trimmed_y)\n",
    "X_train_mi = convert_to(X_train, to_type=\"pd-multiindex\")\n",
    "X_test_mi = convert_to(X_test, to_type='pd-multiindex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sktime.registry import all_tags\n",
    "# from sktime.registry import all_estimators\n",
    "# all_estimators(\"classifier\", filter_tags={\"capability:multivariate\":True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testscale = StandardScaler()\n",
    "scaled = testscale.fit_transform(trimmed_df)\n",
    "scaled = pd.DataFrame(data=scaled, index = trimmed_df.index, columns=trimmed_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'f:\\\\\\\\Master\\\\\\\\Kubernetes\\\\\\\\sockshop\\\\\\\\microservices-demo\\\\\\\\analysis\\\\\\\\storage'\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_csv(scaled, trimmed_y, \"scaled_and_trimmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Try_Classifiers:\n",
    "    #Goal: run the various classifiers. Do Normalization/standarization outside of this function.\n",
    "    #Then in here, only classifiers should be needed. All classifiers implement fit and fit_transform as well as fit_predict.\n",
    "    #If there are multiple classes that passed in, make the pipeline fit_transform -> fit_predict. Otherwise just fit_predict.\n",
    "    \n",
    "    def __init__(self, X_train:pd.DataFrame, y_train=None, X_test=None, y_test=None):\n",
    "        self.formats = [self.format_dflist, self.format_multiindex, self.format_multiindex_manual, self.format_multiindex_dflist]\n",
    "        self.X_train_cols = X_train.columns\n",
    "        self.X_train_index = X_train.index\n",
    "        self.X_train = X_train\n",
    "        if y_train is not None:\n",
    "            self.y_train = y_train\n",
    "        if X_test is not None:\n",
    "            self.X_test = X_test\n",
    "        if y_test is not None:\n",
    "            self.y_test = y_test\n",
    "\n",
    "\n",
    "    def _try_function_with_formats(self, class_function, X, y=None):\n",
    "        latestexception:Exception\n",
    "        try:\n",
    "            result = None\n",
    "            if y is not None:\n",
    "                result = class_function(X, y)\n",
    "            else:\n",
    "                result = class_function(X)\n",
    "            return result, True\n",
    "        except Exception as e:    \n",
    "            latestexception = e      \n",
    "            for fmt in self.formats:\n",
    "                try:\n",
    "                    formatted_X = fmt(X)\n",
    "                    result = None\n",
    "                    if y is not None: \n",
    "                        result = class_function(formatted_X, y)\n",
    "                    else:\n",
    "                        result = class_function(formatted_X)\n",
    "                    return result, True\n",
    "                except Exception as e:\n",
    "                    latestexception = e\n",
    "                    continue\n",
    "            print(latestexception)\n",
    "            return None, False\n",
    "    \n",
    "    def format_dflist(self, input):\n",
    "        return convert_to(input, to_type=\"df-list\")\n",
    "\n",
    "    def format_multiindex(self, input):\n",
    "        return convert_to(input, to_type=\"pd-multiindex\")\n",
    "\n",
    "    def format_multiindex_manual(self, input):\n",
    "        return pd.DataFrame(input, columns=self.X_train_cols, index=self.X_train_index)\n",
    "    \n",
    "    def format_multiindex_dflist(self, input):\n",
    "        return convert_to(pd.DataFrame(input, columns=self.X_train_cols, index=self.X_train_index), to_type=\"df-list\")\n",
    "\n",
    "    def run_fit_predict_single(self, class_to_use):\n",
    "        \n",
    "        result, completed = self._try_function_with_formats(class_to_use.fit,self.X_train, self.y_train)\n",
    "        \n",
    "        if not completed:\n",
    "            return Exception(\"Couldn't fit\")\n",
    "\n",
    "        if hasattr(class_to_use, 'score'):\n",
    "            score, completed = self._try_function_with_formats(class_to_use.predict, self.X_test, self.y_test)\n",
    "        else:\n",
    "            prediction, completed = self._try_function_with_formats(class_to_use.predict, self.X_test)\n",
    "            score = np.mean(prediction == y_test)\n",
    "            \n",
    "        \n",
    "        if not completed:\n",
    "            return Exception(\"Something went wrong\")\n",
    "\n",
    "#It runs the local fit function before sending it to the loop that catches exceptions\n",
    "#The way to fix this is make sure that the fit and predict functions happen inside the loop. It has to handle multiple inputs or just one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance(df:pd.DataFrame) ->pd.Series:\n",
    "    #The coefficient of variation\n",
    "    cv = df.std() / df.mean()\n",
    "    return cv.round(10).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nodejs_eventloop_lag_seconds&edge-router:80&frontend',\n",
       "       'go_memstats_heap_idle_bytes&catalogue:80&catalogue',\n",
       "       'go_memstats_heap_sys_bytes&catalogue:80&catalogue',\n",
       "       'go_goroutines&catalogue:80&catalogue',\n",
       "       'go_memstats_heap_idle_bytes&user:80&user'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def select_by_variance(df:pd.DataFrame, amount:int):\n",
    "    variances:pd.Series = calculate_variance(df)\n",
    "    selection = variances.iloc[0:amount]\n",
    "    return selection.index\n",
    "best_features = select_by_variance(trimmed_df, 5)\n",
    "X = scaled[best_features]\n",
    "best_features\n",
    "#I want to calculate variance on the unscaled data, but select from the scaled data. so just return selection as its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# jsonfile = open(\"./storage/sorted_variance.json\",mode='w')\n",
    "# jsonfile.close()\n",
    "# variances = calculate_variance(trimmed_df)\n",
    "# variances.to_json(jsonfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [('Arsenal', sktime.classification.kernel_based._arsenal.Arsenal),\n",
    "#  ('CNNClassifier', sktime.classification.deep_learning.cnn.CNNClassifier),\n",
    "#  ('CanonicalIntervalForest',\n",
    "#   sktime.classification.interval_based._cif.CanonicalIntervalForest),\n",
    "#  ('Catch22Classifier',\n",
    "#   sktime.classification.feature_based._catch22_classifier.Catch22Classifier),\n",
    "#  ('ColumnEnsembleClassifier',\n",
    "#   sktime.classification.compose._column_ensemble.ColumnEnsembleClassifier),\n",
    "#  ('DrCIF', sktime.classification.interval_based._drcif.DrCIF),\n",
    "#  ('DummyClassifier', sktime.classification.dummy._dummy.DummyClassifier),\n",
    "#  ('FCNClassifier', sktime.classification.deep_learning.fcn.FCNClassifier),\n",
    "#  ('FreshPRINCE',\n",
    "#   sktime.classification.feature_based._fresh_prince.FreshPRINCE),\n",
    "#  ('HIVECOTEV2', sktime.classification.hybrid._hivecote_v2.HIVECOTEV2),\n",
    "#  ('IndividualTDE', sktime.classification.dictionary_based._tde.IndividualTDE),\n",
    "#  ('KNeighborsTimeSeriesClassifier',\n",
    "#   sktime.classification.distance_based._time_series_neighbors.KNeighborsTimeSeriesClassifier),\n",
    "#  ('LSTMFCNClassifier',\n",
    "#   sktime.classification.deep_learning.lstmfcn.LSTMFCNClassifier),\n",
    "#  ('MLPClassifier', sktime.classification.deep_learning.mlp.MLPClassifier),\n",
    "#  ('MUSE', sktime.classification.dictionary_based._muse.MUSE),\n",
    "#  ('ProbabilityThresholdEarlyClassifier',\n",
    "#   sktime.classification.early_classification._probability_threshold.ProbabilityThresholdEarlyClassifier),\n",
    "#  ('RandomIntervalClassifier',\n",
    "#   sktime.classification.feature_based._random_interval_classifier.RandomIntervalClassifier),\n",
    "#  ('ResNetClassifier',\n",
    "#   sktime.classification.deep_learning.resnet.ResNetClassifier),\n",
    "#  ('RocketClassifier',\n",
    "#   sktime.classification.kernel_based._rocket_classifier.RocketClassifier),\n",
    "#  ('ShapeletTransformClassifier',\n",
    "#   sktime.classification.shapelet_based._stc.ShapeletTransformClassifier),\n",
    "#  ('SignatureClassifier',\n",
    "#   sktime.classification.feature_based._signature_classifier.SignatureClassifier),\n",
    "#  ('SummaryClassifier',\n",
    "#   sktime.classification.feature_based._summary_classifier.SummaryClassifier),\n",
    "#  ('TSFreshClassifier',\n",
    "#   sktime.classification.feature_based._tsfresh_classifier.TSFreshClassifier),\n",
    "#  ('TapNetClassifier',\n",
    "#   sktime.classification.deep_learning.tapnet.TapNetClassifier),\n",
    "#  ('TemporalDictionaryEnsemble',\n",
    "#   sktime.classification.dictionary_based._tde.TemporalDictionaryEnsemble),\n",
    "#  ('TimeSeriesSVC', sktime.classification.kernel_based._svc.TimeSeriesSVC),\n",
    "#  ('WeightedEnsembleClassifier',\n",
    "#   sktime.classification.compose._ensemble.WeightedEnsembleClassifier)]\n",
    "from sktime.classification.deep_learning.cnn import CNNClassifier\n",
    "from sktime.classification.interval_based._cif import CanonicalIntervalForest\n",
    "from sktime.classification.feature_based._catch22_classifier import Catch22Classifier\n",
    "from sktime.classification.interval_based._drcif import DrCIF\n",
    "from sktime.classification.deep_learning.fcn import FCNClassifier\n",
    "from sktime.classification.feature_based._fresh_prince import FreshPRINCE\n",
    "from sktime.classification.hybrid._hivecote_v2 import HIVECOTEV2\n",
    "from sktime.classification.dictionary_based._tde import IndividualTDE\n",
    "from sktime.classification.distance_based._time_series_neighbors import KNeighborsClassifier\n",
    "from sktime.classification.deep_learning.lstmfcn import LSTMFCNClassifier\n",
    "from sktime.classification.deep_learning.mlp import MLPClassifier\n",
    "\n",
    "\n",
    "#Often 2 steps: A transform and a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cif = CanonicalIntervalForest()\n",
    "# X_dflist = convert_to(X, to_type=\"df-list\")\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_dflist, trimmed_y)\n",
    "# X_train = convert_to(X_train, to_type='pd-multiindex')\n",
    "# X_test = convert_to(X_test, to_type=\"pd-multiindex\")\n",
    "# testsuite = Try_Classifiers(X_train, y_train, X_test, y_test)\n",
    "# prediction = testsuite.run_fit_predict_single(cif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
