{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sktime\n",
    "import seaborn as snsc\n",
    "import matplotlib.pyplot as plt\n",
    "from convertcsv.import_preprocess_v4 import readcsvs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from sktime.transformations.panel.rocket import MiniRocketMultivariate\n",
    "from sktime.classification.kernel_based import RocketClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.impute import KNNImputer\n",
    "from convertcsv.get_all_metrics_with_tags import get_all_metrics_with_tags\n",
    "from visualization import graphs\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sktime.datatypes import convert_to\n",
    "from sktime.datatypes import MTYPE_REGISTER\n",
    "from collections import Counter\n",
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sktime.classification.compose import ClassifierPipeline\n",
    "from storage import store_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What should we do here?\n",
    "\n",
    "#Set up many different pipelines to compare.\n",
    "\n",
    "#The things to vary, in order of significance/importance\n",
    "\"\"\"\n",
    "NaN treshold\n",
    "\"\"\"\n",
    "file_list, y = get_all_metrics_with_tags(r\"F:\\Master\\Kubernetes\\sockshop\\microservices-demo\\query\\automated\\generated_csvs_4\")\n",
    "initial_readings = readcsvs(file_list, reduce_NaNs_treshold=False, remove_unique_cols=True, remove_monotonic_increasing=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prior testing revealed optimal KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=1)\n",
    "complete_value_set = imputer.fit_transform(initial_readings)\n",
    "imputed_df = pd.DataFrame(complete_value_set, index=initial_readings.index, columns=initial_readings.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the length of each individual time series\n",
    "def trimming(df:pd.DataFrame, y, min_percent=90):\n",
    "    instances = df.index.get_level_values(0).unique()\n",
    "    timeSeriesLengths = []\n",
    "    for instance in instances:\n",
    "        #This line of code gets every second level index value that corresponds to the current first level index value. \n",
    "        second_level_values = df.index.get_level_values(1)[df.index.get_level_values(0)==instance]\n",
    "        length = len(second_level_values)\n",
    "        timeSeriesLengths.append(length)\n",
    "    #Find the most common value.\n",
    "    counts = Counter(timeSeriesLengths)\n",
    "    most_common_value, most_common_value_count = counts.most_common(1)[0]\n",
    "    percentage = (most_common_value_count / len(timeSeriesLengths)) *100\n",
    "\n",
    "    if percentage >= min_percent:\n",
    "        indicies_to_drop = []\n",
    "        for indice in range(len(timeSeriesLengths)):\n",
    "            if timeSeriesLengths[indice] != most_common_value:\n",
    "                indicies_to_drop.append(indice)\n",
    "        filtered_df:pd.DataFrame = df[~df.index.get_level_values(0).isin(indicies_to_drop)]\n",
    "        #Reset the index\n",
    "        current_indice = 0\n",
    "        desired_indice = 0\n",
    "        newindex_tuples = []\n",
    "        for index_tuple in filtered_df.index:\n",
    "        \n",
    "            #nevermind just set it equals, we only care about change anyway\n",
    "            if(index_tuple[0] != current_indice):\n",
    "                #this way, when the index being looked at changes, we know.\n",
    "                current_indice = index_tuple[0]\n",
    "                #we logged that there is a change so we know its time for next indice\n",
    "                desired_indice += 1\n",
    " \n",
    "            newindex_tuples.append((desired_indice, index_tuple[1]))\n",
    "        \n",
    "        newindex = pd.MultiIndex.from_tuples(newindex_tuples, names=filtered_df.index.names)\n",
    "        newdf = pd.DataFrame(filtered_df.values, columns=filtered_df.columns, index=newindex)\n",
    "            \n",
    "        y = np.array(y)\n",
    "\n",
    "        #Now rebuild the multiindex to be monotonically increasing.\n",
    "        \n",
    "        \n",
    "        #This still keeps the fucky index in the thing\n",
    "\n",
    "        return newdf, np.delete(y, indicies_to_drop)\n",
    "\n",
    "    return ValueError(percentage)\n",
    "    #To properly preprocess, one should check if the most common value equals the max value. If it does, great.\n",
    "    #Imagine we find a perfect thing for it. Now we have a couple of datasets consisting of multivariate data.\n",
    "    #It's important to preserve the general shape of the dataset. So just adding mean to the front and back probably isn't that good. \n",
    "    #Can decide to simply purge the ones that aren't of the correct length. To do this, you would have to decide that the most common value is the highest one, and that there are very few time series that differ. Incurs data loss but saves a lot of time.\n",
    "    #Instead of getting bogged down in a custom optimal solution, get the alright one. For now, get a determiner that purging the ones that are too small is ok. \n",
    "    #If ok, proceed. Can then later compare with other algorithms to see if the purging was helpful.\n",
    "    #Write in the thesis that shit happens and time constraints wcyd\n",
    "\n",
    "    #For now, determine if purging is ait\n",
    "    #The most common value should be at least 90% to only tolerate 10% data loss perhaps.\n",
    "\n",
    "trimmed_df , trimmed_y = trimming(imputed_df, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'f:\\\\\\\\Master\\\\\\\\Kubernetes\\\\\\\\sockshop\\\\\\\\microservices-demo\\\\\\\\analysis\\\\\\\\storage'\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_csv.store_csv(trimmed_df, trimmed_y,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
